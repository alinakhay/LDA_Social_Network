{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training notebook\n",
    "\n",
    "Here some last stages of preprocessing and\n",
    "\n",
    "training of LDA and auxillary models such as CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#adding this to avoid memory errors\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 796 stopwords.\n",
      "Got 3738 texts in training corpus.\n"
     ]
    }
   ],
   "source": [
    "#reading stopwords and train set names\n",
    "with open('../Data/stopwords/stopwords.pkl', 'rb') as f:\n",
    "    stopwords = pickle.load(f)\n",
    "with open('../Data/train_names.pkl', 'rb') as f:\n",
    "    train_names = pickle.load(f)\n",
    "print('Got %d stopwords.\\nGot %d texts in training corpus.'%(len(stopwords), len(train_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idf filtering\n",
    "We got no less about 700k of unique words even after deleting numbers and punctuations.\n",
    "\n",
    "So, All the term-doc matrices will have immensly big size like Nx700k, where N is also big.\n",
    "\n",
    "Most of those words are very uninformative and ought to be filtered. So they would be filtered by IDF(Inverse Document Frequency) thresholding.\n",
    "So we can sort out very common/rare words, threshold was set manually and then tuned during few rounds of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of original vocabulary: 718729\n",
      "After filtering: 36450\n"
     ]
    }
   ],
   "source": [
    "#'training' (tf-)idf vectorizer.\n",
    "tf_idf = TfidfVectorizer(input='filename',\n",
    "                             stop_words=stopwords,\n",
    "                             smooth_idf=False\n",
    "                         )\n",
    "tf_idf.fit(train_names)\n",
    "#getting idfs\n",
    "idfs = tf_idf.idf_\n",
    "#sorting out too rare and too common words\n",
    "#original 1.3 and 7\n",
    "# 2 6\n",
    "lower_thresh = 3.\n",
    "upper_thresh = 6.\n",
    "not_often = idfs > lower_thresh\n",
    "not_rare = idfs < upper_thresh\n",
    "\n",
    "mask = not_often * not_rare\n",
    "\n",
    "good_words = np.array(tf_idf.get_feature_names())[mask]\n",
    "#deleting punctuation as well.\n",
    "cleaned = []\n",
    "for word in good_words:\n",
    "    word = re.sub(\"^(\\d+\\w*$|_+)\", \"\", word)\n",
    "    \n",
    "    if len(word) == 0:\n",
    "        continue\n",
    "    cleaned.append(word)\n",
    "print(\"Len of original vocabulary: %d\\nAfter filtering: %d\"%(idfs.shape[0], len(cleaned)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering results\n",
    "\n",
    "Well, we reduced size of vocabulary from 718729 to 36450, that is A LOT.\n",
    "\n",
    "### Stemming\n",
    "\n",
    "We will further reduce size of our vocabulary by stemming russian words.\n",
    "\n",
    "It can be easily done with https://github.com/nlpub/pymystem3\n",
    "which stems russian words without destroying them and ignoring english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cbd0689d794ba988526c0ef7920252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After stemming: 13611\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "m = Mystem()\n",
    "stemmed = set()\n",
    "voc_len = len(cleaned)\n",
    "for i in tqdm(range(voc_len)):\n",
    "    word = cleaned.pop()\n",
    "    stemmed_word = m.lemmatize(word)[0]\n",
    "    stemmed.add(stemmed_word)\n",
    "    \n",
    "stemmed = list(stemmed)\n",
    "print('After stemming: %d'%(len(stemmed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term-doc matrix\n",
    "\n",
    "To construct term-doc matrix we will use CountVectorizer from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training count vectorizer\n",
    "voc = {word : i for i,word in enumerate(stemmed)}\n",
    "\n",
    "count_vect = CountVectorizer(input='filename',\n",
    "                             stop_words=stopwords,\n",
    "                             vocabulary=voc)\n",
    "\n",
    "dataset = count_vect.fit_transform(train_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n",
    "Finally, training LDA.\n",
    "\n",
    "All the hyperparams were set intuitively and tuned for a few rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 30\n",
      "iteration: 2 of max_iter: 30\n",
      "iteration: 3 of max_iter: 30\n",
      "iteration: 4 of max_iter: 30\n",
      "iteration: 5 of max_iter: 30\n",
      "iteration: 6 of max_iter: 30\n",
      "iteration: 7 of max_iter: 30\n",
      "iteration: 8 of max_iter: 30\n",
      "iteration: 9 of max_iter: 30\n",
      "iteration: 10 of max_iter: 30\n",
      "iteration: 11 of max_iter: 30\n",
      "iteration: 12 of max_iter: 30\n",
      "iteration: 13 of max_iter: 30\n",
      "iteration: 14 of max_iter: 30\n",
      "iteration: 15 of max_iter: 30\n",
      "iteration: 16 of max_iter: 30\n",
      "iteration: 17 of max_iter: 30\n",
      "iteration: 18 of max_iter: 30\n",
      "iteration: 19 of max_iter: 30\n",
      "iteration: 20 of max_iter: 30\n",
      "iteration: 21 of max_iter: 30\n",
      "iteration: 22 of max_iter: 30\n",
      "iteration: 23 of max_iter: 30\n",
      "iteration: 24 of max_iter: 30\n",
      "iteration: 25 of max_iter: 30\n",
      "iteration: 26 of max_iter: 30\n",
      "iteration: 27 of max_iter: 30\n",
      "iteration: 28 of max_iter: 30\n",
      "iteration: 29 of max_iter: 30\n",
      "iteration: 30 of max_iter: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=30, mean_change_tol=0.001,\n",
       "             n_components=60, n_jobs=6, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training LDA\n",
    "lda = LDA(n_components = 60, max_iter=30, n_jobs=6, learning_method='batch', verbose=1)\n",
    "lda.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Data/models/tf_idf.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lda, '../Data/models/lda.pkl')\n",
    "joblib.dump(count_vect, '../Data/models/countVect.pkl')\n",
    "joblib.dump(tf_idf,'../Data/models/tf_idf.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
